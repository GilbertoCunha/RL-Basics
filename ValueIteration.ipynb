{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start with a state-value function $v_0(s)$\n",
    "- Iteratively:\n",
    "    - Find the optimal state-value function by iteratively using the Bellman's optimality equation:\n",
    "    \n",
    "$$ v_{k+1}(s) \\leftarrow \\max_{a \\in \\mathcal{A}} \\left( \\mathcal{R}^a_s + \\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{ss^{\\prime}}^{a} v_k(s^{\\prime}) \\right) $$\n",
    "\n",
    "      Or, in vector notation:\n",
    "      \n",
    "$$ \\mathbf{v}_{k+1} \\leftarrow \\max_{a \\in \\mathcal{A}} \\left( \\mathcal{R}^a + \\gamma \\mathcal{P}^{a} \\mathbf{v}_k \\right), \\; \\mathcal{R}^a = \\left[\\mathcal{R}^a_s\\right] \\in \\mathbb{R}^{\\left| \\mathcal{S} \\right|}, \\; \\mathcal{P}^a = \\left[\\mathcal{P}^a_{ss^{\\prime}}\\right] \\in \\mathbb{R}^{\\left| \\mathcal{S} \\right| \\times \\left| \\mathcal{S} \\right|} $$\n",
    "\n",
    "      We should only stop when:\n",
    "      \n",
    "$$\\left| v_{k+1}(s) - v_k(s) \\right| < \\epsilon, \\; \\forall s \\in \\mathcal{S}$$\n",
    "\n",
    "- Finally, after we have discovered the optimal state-value function, we can derive from it the optimal action-value function, from which we can derive the optimal policy:\n",
    "\n",
    "$$ \\pi_{\\star}(s) = \\mathop{\\mathrm{argmax}}_{a \\in \\mathcal{A}} q_{\\star}(s, a) $$\n",
    "\n",
    "$$ q_{\\star}(s, a) = \\mathcal{R}^a_s + \\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}^{a}_{ss^{\\prime}} v_{\\star}(s^{\\prime}) $$\n",
    "\n",
    "    Or, once again in vector notation:\n",
    "    \n",
    "$$ \\pi_{\\star} = \\mathop{\\mathrm{argmax}}_{a \\in \\mathcal{A}} \\left( \\mathcal{R}^a + \\gamma \\mathcal{P}^a \\mathbf{v}_{\\star} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid-World Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a simple problem so that we can apply the above algorithm. This is a simple grid-world, where we want to reach any of the corner squares (the goal). We are allowed to move either up, down, left or right, and every state has a  a reward of $-1$, except of the goal states, regardless of the action taken.\n",
    "\n",
    "Translating this problem into our **Markov Decision Process** $\\mathcal{M} = \\langle \\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma\\rangle$:\n",
    "- $\\mathcal{S}$ is the set of all squares\n",
    "- $\\mathcal{A} = \\{\\mathtt{up}, \\mathtt{down}, \\mathtt{left}, \\mathtt{right}\\}$ if we are not in a goal state, otherwise $\\mathcal{A} = \\{\\}$\n",
    "- $\\mathcal{P}$ is either 1 or 0 given the action and the two states, as this world is fully deterministic\n",
    "- $\\mathcal{R}^a_s = -1, \\forall a \\in \\mathcal{A}$ if $s$ is not a goal state, otherwise $\\mathcal{R}^a_s = 0$\n",
    "- The discount factor $\\gamma \\in \\left[0,1\\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorldMDP:\n",
    "    \n",
    "    def __init__(self, rows, columns, discount):   \n",
    "        # Auxiliary variables\n",
    "        self.rows = rows\n",
    "        self.columns = columns\n",
    "        \n",
    "        # MDP Tuple\n",
    "        self.states = [(i,j) for i in range(rows) for j in range(columns)]\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.probabilities = self.generate_probabilities(self.states, self.actions)\n",
    "        self.rewards = self.generate_rewards(self.states)\n",
    "        self.discount = discount\n",
    "        \n",
    "    def is_goal_state(self, state):\n",
    "        # Get row and column from state\n",
    "        i, j = state\n",
    "        \n",
    "        # Each of the possible goal states\n",
    "        upper_left = (i == 0) and (j == 0)\n",
    "        upper_right = (i == 0) and (j == self.columns-1)\n",
    "        lower_left = (i == self.rows - 1) and (j == 0)\n",
    "        lower_right = (i == self.rows - 1) and (j == self.columns - 1)\n",
    "        \n",
    "        return upper_left or upper_right or lower_left or lower_right\n",
    "    \n",
    "    def get_next_state(self, state, action):\n",
    "        i, j = state\n",
    "        \n",
    "        # Change state according to action\n",
    "        if action == \"up\":\n",
    "            i_, j_ = max(0, i-1), j\n",
    "        elif action == \"down\":\n",
    "            i_, j_ = min(self.rows-1, i+1), j\n",
    "        elif action == \"left\":\n",
    "            i_, j_ = i, max(0, j-1)\n",
    "        elif action == \"right\":\n",
    "            i_, j_ = i, min(self.columns-1, j+1)\n",
    "            \n",
    "        return (i_, j_)\n",
    "    \n",
    "    def get_sucessor_states(self, state):\n",
    "        if self.is_goal_state(state):\n",
    "            s = []\n",
    "        else:\n",
    "            s = [self.get_next_state(state, action) for action in self.actions]\n",
    "        return s\n",
    "    \n",
    "    def generate_probabilities(self, states, actions):\n",
    "        # Populate only the possible transitions and its probabilities\n",
    "        p = {}\n",
    "        \n",
    "        # For each state, populate only the adjacent states with probability 1\n",
    "        for state in states:\n",
    "            if not self.is_goal_state(state):\n",
    "                for action in actions:\n",
    "                    state_ = self.get_next_state(state, action)\n",
    "                    p[(action, state, state_)] = 1\n",
    "                \n",
    "        return p\n",
    "        \n",
    "    def generate_rewards(self, states):\n",
    "        r = {}\n",
    "        \n",
    "        # Populate the probabilities dictionary\n",
    "        for state in states:\n",
    "            if self.is_goal_state(state): r[state] = 0\n",
    "            else: r[state] = -1\n",
    "                \n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define world hyperparameters\n",
    "rows = 10\n",
    "columns = 10\n",
    "discount = 0.9\n",
    "\n",
    "# Generate world\n",
    "MDP = GridWorldMDP(rows, columns, discount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
